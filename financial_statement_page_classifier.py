# -*- coding: utf-8 -*-
"""financial_statement_page_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SAGW5BBsX6yA-25YqeErKuiMZCWa11rF
"""

# Install necessary libraries (run this only once)
!pip install pdfplumber transformers pandas scikit-learn openpyxl

# Import libraries
import os
import pandas as pd
import pdfplumber
import re
from tqdm import tqdm
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

# Define paths
base_path = '/content/drive/MyDrive/Colab Notebooks/TestPipeline'
dataset_path = os.path.join(base_path, 'Dataset', 'cik_list.xlsx')

# Load Excel dataset
df = pd.read_excel(dataset_path)

# Display first few rows
df.head()

# List PDF files
pdf_folder = os.path.join(base_path, 'PDFs')
pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]

# Show list of PDFs
pdf_files

# Function to extract text from each page
def extract_pdf_pages(pdf_path):
    pages_text = []
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                pages_text.append((i + 1, text))  # store page number + text
    return pages_text

# Extract and store all pagesâ€™ text
pdf_text_data = {}
for pdf_file in tqdm(pdf_files):
    full_path = os.path.join(pdf_folder, pdf_file)
    pdf_text_data[pdf_file] = extract_pdf_pages(full_path)

# Show example for one PDF
list(pdf_text_data[pdf_files[0]])[:2]  # first 2 pages

# Example regex rules
def detect_labels(text):
    labels = {'consolidation': None, 'statement': None}
    if re.search(r'standalone', text, re.I):
        labels['consolidation'] = 'Standalone'
    elif re.search(r'consolidated', text, re.I):
        labels['consolidation'] = 'Consolidated'

    if re.search(r'balance sheet', text, re.I):
        labels['statement'] = 'Balance Sheet'
    elif re.search(r'(profit.*loss|p&l)', text, re.I):
        labels['statement'] = 'P&L'
    elif re.search(r'cash flow', text, re.I):
        labels['statement'] = 'Cash Flow'

    return labels

# Apply to all pages
page_records = []
for pdf_file, pages in pdf_text_data.items():
    for page_num, text in pages:
        labels = detect_labels(text)
        page_records.append({
            'pdf_file': pdf_file,
            'page_num': page_num,
            'text': text,
            'consolidation': labels['consolidation'],
            'statement': labels['statement']
        })

# Convert to DataFrame
pages_df = pd.DataFrame(page_records)
pages_df.head()

# For demonstration, use zero-shot classification
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Define labels
statement_labels = ['Balance Sheet', 'P&L', 'Cash Flow']
consolidation_labels = ['Standalone', 'Consolidated']

# Function to apply zero-shot classification
def classify_page(text, candidate_labels):
    result = classifier(text, candidate_labels)
    return result['labels'][0]  # top prediction

# Apply to unlabeled pages (fallback)
for idx, row in pages_df.iterrows():
    if pd.isna(row['consolidation']):
        pages_df.at[idx, 'consolidation'] = classify_page(row['text'], consolidation_labels)
    if pd.isna(row['statement']):
        pages_df.at[idx, 'statement'] = classify_page(row['text'], statement_labels)

pages_df[['pdf_file', 'page_num', 'consolidation', 'statement']].head()

# Group page numbers by category
result = {}
for (pdf, cons_type, stmt_type), group in pages_df.groupby(['pdf_file', 'consolidation', 'statement']):
    if pdf not in result:
        result[pdf] = {}
    if cons_type not in result[pdf]:
        result[pdf][cons_type] = {}
    result[pdf][cons_type][stmt_type] = list(group['page_num'])

# Display structured result
import json
print(json.dumps(result, indent=2))

# Save full DataFrame
output_path = os.path.join(base_path, 'page_classification_results.csv')
pages_df.to_csv(output_path, index=False)
print(f'Results saved to {output_path}')

df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TestPipeline/page_classification_results.csv')
df2.head()

