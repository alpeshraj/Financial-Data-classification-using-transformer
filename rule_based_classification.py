# -*- coding: utf-8 -*-
"""rule_based_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SaIULa2RIwn1fGbFycfaGLTL1LhALxvG
"""

!pip install PyPDF2 pdfplumber scikit-learn pandas

# Import libraries
import PyPDF2
import pdfplumber
import re
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import joblib
import io
from google.colab import files

# @title PDF Text Extraction
def extract_text_from_pdf(pdf_path):
    """
    Robust text extraction using both PyPDF2 and pdfplumber
    """
    full_text = []

    # First pass with PyPDF2
    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            full_text.append(page.extract_text() or "")  # Ensure no None values

    # Second pass with pdfplumber for more accurate extraction
    detailed_text = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            detailed_text.append(page.extract_text() or "")

    # Combine results for best accuracy
    combined_text = []
    for i in range(len(full_text)):
        if len(detailed_text[i]) > len(full_text[i]):
            combined_text.append(detailed_text[i])
        else:
            combined_text.append(full_text[i])

    return combined_text

# @title Financial Statement Classifier
class FinancialStatementClassifier:
    def __init__(self):
        self.consolidation_model = None
        self.statement_model = None
        self.consolidation_vectorizer = None
        self.statement_vectorizer = None

    def train(self, texts, consolidation_labels, statement_labels):
        """Train both classification models"""
        # Consolidation classifier (standalone vs consolidated)
        self.consolidation_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        X_consolidation = self.consolidation_vectorizer.fit_transform(texts)
        self.consolidation_model = RandomForestClassifier(n_estimators=100)
        self.consolidation_model.fit(X_consolidation, consolidation_labels)

        # Statement type classifier
        self.statement_vectorizer = TfidfVectorizer(max_features=800, stop_words='english')
        X_statement = self.statement_vectorizer.fit_transform(texts)
        self.statement_model = RandomForestClassifier(n_estimators=100)
        self.statement_model.fit(X_statement, statement_labels)

    def predict(self, text):
        """Predict both consolidation status and statement type"""
        if not self.consolidation_model:
            raise Exception("Model not trained")

        # Predict consolidation status
        consolidation_features = self.consolidation_vectorizer.transform([text])
        consolidation = self.consolidation_model.predict(consolidation_features)[0]

        # Predict statement type
        statement_features = self.statement_vectorizer.transform([text])
        statement_type = self.statement_model.predict(statement_features)[0]

        return consolidation, statement_type

    def save_models(self, filename="financial_classifier.joblib"):
        """Save models to file"""
        joblib.dump({
            'consolidation_model': self.consolidation_model,
            'statement_model': self.statement_model,
            'consolidation_vectorizer': self.consolidation_vectorizer,
            'statement_vectorizer': self.statement_vectorizer
        }, filename)
        return filename

    def load_models(self, filename):
        """Load models from file"""
        models = joblib.load(filename)
        self.consolidation_model = models['consolidation_model']
        self.statement_model = models['statement_model']
        self.consolidation_vectorizer = models['consolidation_vectorizer']
        self.statement_vectorizer = models['statement_vectorizer']

# @title Rule-Based Fallbacks
def rule_based_consolidation(text):
    """Fallback consolidation detection"""
    text = text.lower()
    standalone_keywords = ['standalone', 'individual', 'separate', 'single']
    consolidated_keywords = ['consolidated', 'combined', 'group', 'merged']

    standalone_score = sum(text.count(kw) for kw in standalone_keywords)
    consolidated_score = sum(text.count(kw) for kw in consolidated_keywords)

    return 'standalone' if standalone_score > consolidated_score else 'consolidated'

def rule_based_statement_type(text):
    """Fallback statement type detection"""
    text = text.lower()

    balance_sheet_keywords = ['balance sheet', 'statement of financial position',
                             'assets', 'liabilities', 'equity', 'capital']
    profit_loss_keywords = ['profit and loss', 'income statement', 'statement of operations',
                          'revenue', 'expenses', 'net income', 'ebitda']
    cash_flow_keywords = ['cash flow', 'statement of cash flows',
                         'operating activities', 'investing activities', 'financing activities']

    scores = {
        'balance_sheet': sum(text.count(kw) for kw in balance_sheet_keywords),
        'profit_loss': sum(text.count(kw) for kw in profit_loss_keywords),
        'cash_flow': sum(text.count(kw) for kw in cash_flow_keywords)
    }

    # Return the type with highest score, default to balance sheet if no clear match
    return max(scores.items(), key=lambda x: x[1])[0] if max(scores.values()) > 0 else 'balance_sheet'

# @title PDF Processing Pipeline
def process_financial_pdf(pdf_path, classifier=None, use_rules=True):
    """
    Process PDF and classify each page
    Returns dictionary with page numbers for each category
    """
    # Extract text from all pages
    pages = extract_text_from_pdf(pdf_path)

    results = {
        'standalone': {'balance_sheet': [], 'profit_loss': [], 'cash_flow': []},
        'consolidated': {'balance_sheet': [], 'profit_loss': [], 'cash_flow': []}
    }

    for i, page_text in enumerate(pages):
        # Skip empty or nearly empty pages
        if len(page_text.strip()) < 100:
            continue

        clean_text = ' '.join(page_text.split()).lower()

        try:
            if classifier:
                consolidation, stmt_type = classifier.predict(clean_text)
            elif use_rules:
                consolidation = rule_based_consolidation(clean_text)
                stmt_type = rule_based_statement_type(clean_text)
            else:
                raise Exception("No classification method available")

            # Store results (page numbers are 1-based)
            results[consolidation][stmt_type].append(i+1)
        except Exception as e:
            print(f"Error processing page {i+1}: {str(e)}")
            continue

    return results

# @title Main Execution
def main():
    # Upload PDF file
    print("Please upload your financial PDF file:")
    uploaded = files.upload()
    pdf_filename = next(iter(uploaded))

    # Initialize classifier (in a real scenario, you'd load a pre-trained model)
    classifier = FinancialStatementClassifier()

    # Process with rule-based approach (since we likely don't have a trained model)
    print("\nProcessing PDF with rule-based classification...")
    results = process_financial_pdf(pdf_filename, use_rules=True)

    # Display results in a nice format
    print("\nClassification Results:")
    print("="*40)
    for consolidation_type in results:
        print(f"\n{consolidation_type.upper()} STATEMENTS:")
        for stmt_type, pages in results[consolidation_type].items():
            if pages:
                print(f"- {stmt_type.replace('_', ' ').title()}: Pages {', '.join(map(str, pages))}")

    # Option to download results as CSV
    result_df = pd.DataFrame([(consolidation, stmt, page)
                            for consolidation in results
                            for stmt in results[consolidation]
                            for page in results[consolidation][stmt]],
                            columns=['Consolidation', 'Statement Type', 'Page Number'])

    csv_filename = pdf_filename.replace('.pdf', '_results.csv')
    result_df.to_csv(csv_filename, index=False)
    print(f"\nResults saved to {csv_filename}")
    files.download(csv_filename)

if __name__ == "__main__":
    main()

